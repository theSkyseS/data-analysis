{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "\"Обработка ЕЯ, основы NLP 1.ipynb\"",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1T45drVGAoZS"
   },
   "source": [
    "Задание:\n",
    "\n",
    "С помощью метода get_prepared_all_vacancies_from_hh\n",
    "загрузить 10000 вакансии с сайта hh.ru.\n",
    "\n",
    "Договоритесь чтобы вакансии не повторялись.\n",
    "\n",
    "Сохранить результат в виде csv файла с тремя столбцами:\n",
    "1. id вакансии\n",
    "2. сырой json\n",
    "3. подготовленное описание вакансии\n",
    "\n",
    "Не забывайте про обработку ошибок"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oSGoAoSOsaOr"
   },
   "source": [
    "import pickle\n",
    "import nltk\n",
    "import pymystem3\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "def download_json(url):\n",
    "    response = urlopen(url)\n",
    "    data_json = json.loads(response.read())\n",
    "    print(data_json['error'])\n",
    "    return data_json\n",
    "\n",
    "# пример: download_json('https://api.hh.ru/vacancies/43551857')"
   ],
   "execution_count": 89,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Andrew\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Andrew\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Andrew\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TR80d6BquBcm"
   },
   "source": [
    "'''\n",
    "РЕАЛИЗОВАТЬ remove_garbage\n",
    "Используйте регулярные выражения\n",
    "IN: str '<p>ТРЕБОВАНИЯ: <br /> - обязательно высшее или незаконченное высшее экономическое образование;...'\n",
    "OUT: str 'ТРЕБОВАНИЯ: - обязательно высшее или незаконченное высшее экономическое образование;...'\n",
    "'''\n",
    "import re\n",
    "\n",
    "\n",
    "def remove_garbage(raw_text):\n",
    "    text = raw_text\n",
    "    text = re.sub(\"<[^>]*>\", \"\", text)\n",
    "    text = re.sub(\"&quot;\", \"\", text)\n",
    "    return text"
   ],
   "execution_count": 85,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0trYWVvtIkOW"
   },
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "'''\n",
    "РЕАЛИЗОВАТЬ tokenize\n",
    "Используйте word_tokenize из nltk.tokenize\n",
    "IN: str 'ТРЕБОВАНИЯ: - обязательно высшее или незаконченное высшее экономическое образование;...'\n",
    "OUT: list ['требования', ':', '-', 'обязательно', 'высшее', 'или', 'незаконченное', 'высшее', 'экономическое', 'образование']\n",
    "'''\n",
    "\n",
    "\n",
    "def tokenize(raw_text):\n",
    "    tokens = word_tokenize(raw_text, language=\"russian\")\n",
    "    return tokens"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Jp-S1920L6QT"
   },
   "source": [
    "from spacy import load\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "'''\n",
    "РЕАЛИЗОВАТЬ to_base_form\n",
    "Можно использовать и лемматизацию и стемминг: pymystem3, ntlk, Spacy, gensim\n",
    "IN: str 'Красивая мама красиво мыла раму'\n",
    "OUT: str 'красивый мама красиво мыть рама'\n",
    "'''\n",
    "\n",
    "\n",
    "def to_base_form(raw_text):\n",
    "    tokens = []\n",
    "    lemmer = SnowballStemmer(\"russian\")\n",
    "    # lemmer = load('ru_core_news_md', disable='tok2vec')\n",
    "    for word in raw_text:\n",
    "        token = lemmer.stem(word)\n",
    "        tokens.append(token)\n",
    "    return tokens"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2FCOqsQPuxQE"
   },
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "'''\n",
    "РЕАЛИЗОВАТЬ remove_stop_words\n",
    "Используйте stopwords из nltk.corpus \n",
    "IN: list ['требования', ':', '-', 'обязательно', 'высший', 'или', 'незаконченный', 'высший', 'экономический', 'образование']\n",
    "OUT: list ['требование', 'обязательно', 'высший', 'незаконченный', 'высший', 'экономический', 'образование']\n",
    "'''\n",
    "\n",
    "\n",
    "def remove_stop_words(raw_words):\n",
    "    stop_words = set(stopwords.words('russian'))\n",
    "    prep_signs = {\",\", \".\", \":\", \";\", \"-\", \"_\", \"\\'\", \"\\\"\", \"/\", \"\\\\\", \"+\", \"=\", \"(\", \")\"}\n",
    "    stop_words = stop_words.union(prep_signs)\n",
    "    filtered_words = []\n",
    "    for word in raw_words:\n",
    "        if word not in stop_words:\n",
    "            filtered_words.append(word)\n",
    "    return filtered_words"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HV4CVKnDvFil"
   },
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "'''\n",
    "РЕАЛИЗОВАТЬ get_prepared_vacancy_description_from_hh\n",
    "Используйте реализованные ранее методы\n",
    "IN: int 72323\n",
    "OUT1: str vacancy_json dump\n",
    "OUT2: list[list] [['требование', 'обязательно', 'высший', 'незаконченный', 'высший', 'экономический', 'образование', ...], [...], ...]\n",
    "'''\n",
    "\n",
    "\n",
    "def get_prepared_vacancy_from_hh(vacancy_id):\n",
    "    base_url = 'https://api.hh.ru/vacancies/'\n",
    "    #try:\n",
    "    vacancy_json = download_json(base_url + str(vacancy_id))\n",
    "    #except HTTPError as e:\n",
    "    #raise Exception(e.code)\n",
    "    vacancy_description = vacancy_json['description']  # получить описание вакансии из json\n",
    "    vacancy_description_sentences = sent_tokenize(vacancy_description, \"russian\")\n",
    "    prepared_vacancy_description = []\n",
    "    for sentence in vacancy_description_sentences:\n",
    "        sentence = remove_garbage(sentence)\n",
    "        description = tokenize(sentence)\n",
    "        description = to_base_form(description)\n",
    "        description = remove_stop_words(description)\n",
    "        #print(description)\n",
    "        for desc in description:\n",
    "            prepared_vacancy_description.append(desc)\n",
    "    return vacancy_json, prepared_vacancy_description"
   ],
   "execution_count": 77,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4CBLmNNKDFWZ"
   },
   "source": [
    "from urllib.error import HTTPError\n",
    "\n",
    "'''\n",
    "РЕАЛИЗОВАТЬ get_prepared_all_vacancies_from_hh\n",
    "Используйте get_prepared_vacancy_from_hh и ThreadPoolExecutor из concurrent.futures\n",
    "IN: list [int]\n",
    "OUT: list [vacancy_id, raw_description, prepared_description]\n",
    "'''\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "def get_prepared_all_vacancies_from_hh(vacancy_ids):\n",
    "    executor = ThreadPoolExecutor(max_workers=16)\n",
    "    future_list = []\n",
    "    for vacancy_id in vacancy_ids:\n",
    "        try:\n",
    "            future = executor.submit(get_prepared_vacancy_from_hh, vacancy_id)\n",
    "            future_list.append(future)\n",
    "        except URLError:\n",
    "            continue\n",
    "        except TimeoutError:\n",
    "            continue\n",
    "        except Exception:\n",
    "           continue\n",
    "\n",
    "    result = []\n",
    "    for future in future_list:\n",
    "        future_result = future.result()\n",
    "        result.append(future_result)\n",
    "    return result"
   ],
   "execution_count": 87,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.error import HTTPError, URLError\n",
    "\n",
    "#try:\n",
    "#pass\n",
    "#with open('data.pickle', 'rb') as f:\n",
    "#s = pickle.load(f)\n",
    "#except FileNotFoundError:\n",
    "#s = []\n",
    "s = []\n",
    "try:\n",
    "    vacancy_ids = range(100, 301)\n",
    "    s.append(get_prepared_all_vacancies_from_hh(vacancy_ids))\n",
    "except Exception as e:\n",
    "    # print(e)\n",
    "    pass\n",
    "with open('data.pickle', 'wb') as f:\n",
    "    pickle.dump(s, f)\n",
    "s"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}