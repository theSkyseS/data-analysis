{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model, model_selection, metrics, \\\n",
    "    preprocessing, neighbors, tree, naive_bayes, svm, pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "   Cultivar  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n0         1    14.23        1.71  2.43               15.6        127   \n1         1    13.20        1.78  2.14               11.2        100   \n2         1    13.16        2.36  2.67               18.6        101   \n3         1    14.37        1.95  2.50               16.8        113   \n4         1    13.24        2.59  2.87               21.0        118   \n\n   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n0           2.80        3.06                  0.28             2.29   \n1           2.65        2.76                  0.26             1.28   \n2           2.80        3.24                  0.30             2.81   \n3           3.85        3.49                  0.24             2.18   \n4           2.80        2.69                  0.39             1.82   \n\n   Color intensity   Hue  OD280/OD315  Proline  \n0             5.64  1.04         3.92     1065  \n1             4.38  1.05         3.40     1050  \n2             5.68  1.03         3.17     1185  \n3             7.80  0.86         3.45     1480  \n4             4.32  1.04         2.93      735  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cultivar</th>\n      <th>Alcohol</th>\n      <th>Malic acid</th>\n      <th>Ash</th>\n      <th>Alcalinity of ash</th>\n      <th>Magnesium</th>\n      <th>Total phenols</th>\n      <th>Flavanoids</th>\n      <th>Nonflavanoid phenols</th>\n      <th>Proanthocyanins</th>\n      <th>Color intensity</th>\n      <th>Hue</th>\n      <th>OD280/OD315</th>\n      <th>Proline</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>14.23</td>\n      <td>1.71</td>\n      <td>2.43</td>\n      <td>15.6</td>\n      <td>127</td>\n      <td>2.80</td>\n      <td>3.06</td>\n      <td>0.28</td>\n      <td>2.29</td>\n      <td>5.64</td>\n      <td>1.04</td>\n      <td>3.92</td>\n      <td>1065</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>13.20</td>\n      <td>1.78</td>\n      <td>2.14</td>\n      <td>11.2</td>\n      <td>100</td>\n      <td>2.65</td>\n      <td>2.76</td>\n      <td>0.26</td>\n      <td>1.28</td>\n      <td>4.38</td>\n      <td>1.05</td>\n      <td>3.40</td>\n      <td>1050</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>13.16</td>\n      <td>2.36</td>\n      <td>2.67</td>\n      <td>18.6</td>\n      <td>101</td>\n      <td>2.80</td>\n      <td>3.24</td>\n      <td>0.30</td>\n      <td>2.81</td>\n      <td>5.68</td>\n      <td>1.03</td>\n      <td>3.17</td>\n      <td>1185</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>14.37</td>\n      <td>1.95</td>\n      <td>2.50</td>\n      <td>16.8</td>\n      <td>113</td>\n      <td>3.85</td>\n      <td>3.49</td>\n      <td>0.24</td>\n      <td>2.18</td>\n      <td>7.80</td>\n      <td>0.86</td>\n      <td>3.45</td>\n      <td>1480</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>13.24</td>\n      <td>2.59</td>\n      <td>2.87</td>\n      <td>21.0</td>\n      <td>118</td>\n      <td>2.80</td>\n      <td>2.69</td>\n      <td>0.39</td>\n      <td>1.82</td>\n      <td>4.32</td>\n      <td>1.04</td>\n      <td>2.93</td>\n      <td>735</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загружаем обработанный набор данных\n",
    "wine_df = pd.read_csv(\"../data/wine_preprocessed.csv\")\n",
    "wine_df = wine_df.drop(columns=\"Unnamed: 0\")\n",
    "wine_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Отделяем ключевой атрибут\n",
    "X = wine_df.drop(columns='Cultivar').values\n",
    "y = wine_df.iloc[:, :1].values\n",
    "y = y.ravel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Разделяем выборку на обучение и тест\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# оценка модели без подбора параметров\n",
    "def initial_evaluation(estimator):\n",
    "    estimator.fit(X_train, y_train)\n",
    "    y_pred = estimator.predict(X_test)\n",
    "    # кросс-валидация\n",
    "    score = model_selection.cross_val_score(estimator, X, y, cv=3)\n",
    "    print('Confusion matrix:\\n', metrics.confusion_matrix(y_test, y_pred),'\\n')\n",
    "    print('Classification report:\\n', metrics.classification_report(y_test,  y_pred), '\\n')\n",
    "    print(score.mean())\n",
    "\n",
    "# подбор параметров\n",
    "def gridSearchResult(estimator, param_grid):\n",
    "    print('Given parameters: ', param_grid)\n",
    "    grid_search = model_selection.GridSearchCV(estimator, param_grid, scoring='f1_micro', cv=3)\n",
    "    grid_search.fit(X, y)\n",
    "    print('Best params: ', grid_search.best_params_)\n",
    "    print('F1 score: ', grid_search.best_score_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Объявляем StandardScaler для нормализации данных\n",
    "\n",
    "scaler = preprocessing.StandardScaler()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Метод k-ближайших соседей (K-Nearest Neighbors)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors:\n",
      "\n",
      "Confusion matrix:\n",
      " [[14  0  0]\n",
      " [ 0 18  0]\n",
      " [ 0  0 13]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        14\n",
      "           2       1.00      1.00      1.00        18\n",
      "           3       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n",
      "0.9439736346516008\n",
      "Given parameters:  {'scaler': ['passthrough', StandardScaler()], 'KNC__n_neighbors': [3, 5, 10, 15, 20], 'KNC__p': [1, 2], 'KNC__weights': ['uniform', 'distance']}\n",
      "Best params:  {'KNC__n_neighbors': 3, 'KNC__p': 1, 'KNC__weights': 'uniform', 'scaler': StandardScaler()}\n",
      "F1 score:  0.9608286252354049\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "KN_classifier = neighbors.KNeighborsClassifier()\n",
    "KN_param_grid = dict(scaler=['passthrough', scaler] ,KNC__n_neighbors=[3, 5, 10, 15, 20], KNC__p=[1, 2], KNC__weights=['uniform', 'distance'])\n",
    "KN_pipe = pipeline.Pipeline(steps=[('scaler', scaler), ('KNC', KN_classifier)])\n",
    "\n",
    "print('K-Nearest Neighbors:\\n')\n",
    "initial_evaluation(KN_pipe)\n",
    "gridSearchResult(KN_pipe, KN_param_grid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Классификатор дерева решений (Decision Tree Classifier)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier:\n",
      "\n",
      "Confusion matrix:\n",
      " [[12  1  1]\n",
      " [ 0 17  1]\n",
      " [ 0  2 11]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.86      0.92        14\n",
      "           2       0.85      0.94      0.89        18\n",
      "           3       0.85      0.85      0.85        13\n",
      "\n",
      "    accuracy                           0.89        45\n",
      "   macro avg       0.90      0.88      0.89        45\n",
      "weighted avg       0.90      0.89      0.89        45\n",
      "\n",
      "0.87090395480226\n",
      "Given parameters:  {'tree__max_depth': [2, 5, 10, 20, 50, 100, None], 'tree__criterion': ['gini', 'entropy'], 'tree__min_samples_leaf': [1, 2, 3, 4, 5, 10]}\n",
      "Best params:  {'tree__criterion': 'gini', 'tree__max_depth': 5, 'tree__min_samples_leaf': 3}\n",
      "F1 score:  0.9099811676082862\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/tree.html\n",
    "tree_classifier = tree.DecisionTreeClassifier()\n",
    "tree_pipe = pipeline.Pipeline(steps=[('scaler', scaler), ('tree', tree_classifier)])\n",
    "tree_param_grid = dict(tree__max_depth=[2, 5, 10, 20, 50, 100, None], tree__criterion=['gini', 'entropy'], tree__min_samples_leaf=[1, 2, 3, 4, 5, 10])\n",
    "print('Decision Tree Classifier:\\n')\n",
    "initial_evaluation(tree_pipe)\n",
    "gridSearchResult(tree_pipe, tree_param_grid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Наивный байесовский классификатор (Naive Bayes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes:\n",
      "\n",
      "Confusion matrix:\n",
      " [[13  1  0]\n",
      " [ 0 18  0]\n",
      " [ 0  0 13]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.93      0.96        14\n",
      "           2       0.95      1.00      0.97        18\n",
      "           3       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.98      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n",
      "0.9607344632768361\n",
      "Given parameters:  {'bayes__var_smoothing': [1e-09, 1e-12, 1e-05]}\n",
      "Best params:  {'bayes__var_smoothing': 1e-09}\n",
      "F1 score:  0.9607344632768361\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\n",
    "NB_classifier = naive_bayes.GaussianNB()\n",
    "NB_pipe = pipeline.Pipeline(steps=[('scaler', scaler), ('bayes', NB_classifier)])\n",
    "NB_param_grid = dict(bayes__var_smoothing=[1e-09, 1e-12, 1e-05])\n",
    "print('Naive Bayes:\\n')\n",
    "initial_evaluation(NB_pipe)\n",
    "gridSearchResult(NB_pipe, NB_param_grid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Метод опорных векторов (Support Vector Machines)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/svm.html\n",
    "SVM_classifier = svm.SVC()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machines:\n",
      "\n",
      "Confusion matrix:\n",
      " [[13  1  0]\n",
      " [ 0 18  0]\n",
      " [ 0  0 13]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.93      0.96        14\n",
      "           2       0.95      1.00      0.97        18\n",
      "           3       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.98      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n",
      "0.9830508474576272\n",
      "Given parameters:  {'SVM__C': [0.2, 0.5, 0.7, 1.0, 1.2, 1.5, 1.7, 2, 2.5], 'SVM__kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'SVM__tol': [0.1, 0.01, 0.001, 0.0001, 1e-05], 'SVM__probability': [True, False]}\n",
      "Best params:  {'SVM__C': 2.5, 'SVM__kernel': 'rbf', 'SVM__probability': True, 'SVM__tol': 0.1}\n",
      "F1 score:  0.9887005649717514\n"
     ]
    }
   ],
   "source": [
    "SVM_pipe = pipeline.Pipeline(steps=[('scaler', scaler), ('SVM', SVM_classifier)])\n",
    "SVM_param_grid = dict(SVM__C=[0.2, 0.5, 0.7, 1.0, 1.2, 1.5, 1.7, 2, 2.5], SVM__kernel=['linear', 'poly', 'rbf', 'sigmoid'], SVM__tol=[0.1, 0.01, 0.001, 0.0001, 0.00001], SVM__probability=[True, False])\n",
    "print('Support Vector Machines:\\n')\n",
    "initial_evaluation(SVM_pipe)\n",
    "gridSearchResult(SVM_pipe, SVM_param_grid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Логистическая регрессия (Logistic Regression)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "\n",
      "Confusion matrix:\n",
      " [[14  0  0]\n",
      " [ 0 18  0]\n",
      " [ 0  1 12]]\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        14\n",
      "           2       0.95      1.00      0.97        18\n",
      "           3       1.00      0.92      0.96        13\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.98      0.97      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n",
      "0.9719397363465161\n",
      "Given parameters:  {'LR__C': [0.001, 0.01, 0.1, 0.3, 1, 1.2, 1.5], 'LR__solver': ['liblinear', 'lbfgs', 'newton-cg', 'saga'], 'LR__tol': [0.1, 0.01, 0.001, 0.0001, 1e-05], 'LR__max_iter': [1000]}\n",
      "Best params:  {'LR__C': 0.3, 'LR__max_iter': 1000, 'LR__solver': 'saga', 'LR__tol': 0.1}\n",
      "F1 score:  0.9775894538606403\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.htm\n",
    "LR_classifier = linear_model.LogisticRegression()\n",
    "LR_pipe = pipeline.Pipeline(steps=[('scaler', scaler), ('LR', LR_classifier)])\n",
    "LR_param_grid = dict(LR__C=[0.001, 0.01, 0.1, 0.3, 1, 1.2, 1.5], LR__solver=['liblinear', 'lbfgs', 'newton-cg', 'saga'], LR__tol=[0.1, 0.01, 0.001, 0.0001, 0.00001], LR__max_iter=[1000])\n",
    "print('Logistic Regression:\\n')\n",
    "initial_evaluation(LR_pipe)\n",
    "gridSearchResult(LR_pipe, LR_param_grid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Вывод\n",
    "\n",
    "Все модели имеют оценку выше, чем 0.9, однако лучше всех отработали Логистическая регрессия и Метод опорных векторов; сильно хуже отработало Дерево решений."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}